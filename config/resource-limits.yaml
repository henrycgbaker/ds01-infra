# DS01 GPU Server - Resource Limits Configuration
# /opt/ds01-infra/config/resource-limits.yaml

# This file controls default and per-user/group resource allocations

# Default limits for all users (unless overridden below)
defaults:
  gpus: 1              # Number of GPUs (or "all" for no limit)
  gpu_memory: null     # GPU memory limit in GB (null = no limit)
  cpus: 16              # CPU cores
  memory: 32g          # RAM limit
  memory_swap: 32g     # Swap limit (should match memory to prevent swap)
  shm_size: 16g        # Shared memory (important for PyTorch DataLoader)
  pids_limit: 4096     # Max processes (prevents fork bombs)
  idle_timeout: 48h    # Auto-stop after X hours of inactivity
  max_tasks: 512 

default_group: student

# User group definitions

groups:
  student:
    members: []  # TO DO need to autopopulate from LDAP group 'ds01-students'
    # CPU/RAM limits (enforced by cgroups)
    cpus: 16
    memory: 32g
    max_tasks: 512
    
    # GPU policy (enforced by wrapper)
    max_gpus_per_user: 1        # can't hog multiple GPUs
    max_concurrent_containers: 2 # can have CPU-only + GPU container
    gpu_time_limit: 48h         # auto-stop after 48h
    gpu_idle_threshold: 10      # stop if <10% util for 2h
    
  researcher:
    members: [] 
    cpus: 32
    memory: 64g
    max_tasks: 2048
    max_gpus_per_user: 2
    max_concurrent_containers: 4
    gpu_time_limit: 168h
    gpu_idle_threshold: 5
    
  admin:
    members: [] 
    members: [datasciencelab]  
    gpus: all    
    cpus: 64
    memory: 128g
    memory_swap: 128g
    shm_size: 64g
    idle_timeout: null  # No timeout
    max_tasks: null

# Per-user overrides (takes precedence over group settings)
user_overrides:
  # Example:
  # john_doe:
  #   gpus: 2
  #   memory: 64g
  #   reason: "Thesis work - approved by supervisor"

# GPU allocation strategy
gpu_allocation:
  strategy: round_robin  # Options: round_robin, least_used, manual
  pin_gpu: true         # Enforce GPU pinning 
  allow_multi_gpu: false  # Allow single user to span multiple GPUs (for researchers only)

# Container lifecycle policies
policies:
  auto_stop_idle: true
  checkpoint_warning: 36h  # Warn users X hours before auto-stop
  require_workspace_mount: true  # Force all containers to mount /workspace
  max_containers_per_user: 5
  
# Advanced settings
advanced:
  enable_mig: true
  enable_mps: true 
  network_mode: host  # or 'bridge'
  restart_policy: unless-stopped